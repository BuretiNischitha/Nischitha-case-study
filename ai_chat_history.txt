Draconic Case Study (Minimal Version)

Project: Customer Support Ticket Analyzer (Multi-Agent System)
Goal: Use Pydantic AI to build Sentiment + Routing agents with prompt engineering, safe JSON parsing, and LLM-based logic.

Initial Setup:

User: I don’t know anything about GitHub Pydantic AI. Help me set up everything.

Assistant: Guided user step-by-step through repo setup, file creation, and folder structure.

Created folders/files:

agents/schemas.py

agents/base_agent.py

agents/sentiment_agent.py

agents/routing_agent.py

evaluation/test_cases.py

evaluation/run_tests.py

evaluation/metrics.py

main.py

docs/architecture.md

README.md

Agent Design:

Introduced AIAgent base class (Generic with input/output Pydantic models).

SentimentAgent: Extracts sentiment, urgency, emotional intensity from ticket.

RoutingAgent: Uses combined ticket + sentiment data to assign priority, escalation, team.

Prompt Iterations:

Added strict instruction: "Respond with JSON only. No markdown."

Example JSON outputs in prompt improved LLM consistency.

Used json.loads() to parse safely instead of eval().

Test Flow:

Used TicketInput to define 5 real-world tickets in test_cases.py

Passed each ticket through SentimentAgent and then RoutingAgent.

Results printed and validated with expected format.

Debugging & Fixes:

OpenAI RateLimit → switched to OpenRouter.

Parsing errors → sliced from first { and used json.loads().

KeyErrors → resolved by matching prompt keys with model fields.

Documentation:

README.md: Clean explanation of problem, solution, structure, and run steps.

architecture.md: Justified agent separation, prompt strategy, and test case behavior.

Final Outcome:

Fully working modular multi-agent AI system

5 test cases passed

Clean, validated output

GitHub repo structured and documented professionally
